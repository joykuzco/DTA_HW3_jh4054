---
title: "p8105_hw3_jh4054"
author: "Joy Hsu"
date: "10/12/2018"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
  
---

This R Markdown for HW3 reinforces skills from Data Visualization and Exploratory Data Analysis

# Setup

Load packages and set chunk options

```{r setup, collapse=TRUE}
library(tidyverse)
library(patchwork)
library(viridis)
library(janitor)
library(p8105.datasets)

knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE)
```

# Problem 1

Import and clean dataset
```{r}
brfss = p8105.datasets::brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(
    -class, -topic, -question, -sample_size, 
    -c(confidence_limit_low:geo_location)) %>% 
  separate(locationdesc, into = c("rem1", "location"), sep = 5) %>% 
  select(-rem1) %>% 
  rename(state = locationabbr, proportion = data_value) %>% 
  mutate(response = factor(response, levels = c("Excellent","Very good", "Good", "Fair", "Poor"))) %>% 
  arrange(year, state, location)
```

### Part 1.1 

In 2002, responses for the Overall Health parameter was available for exactly 7 locations in 3 states: Connecticut, Florida, and North Carolina.

```{r}
brfss %>% 
  filter(year == 2002) %>% 
  distinct(state, location) %>% 
  group_by(state) %>% 
  summarise(number_locations = n()) %>% 
  filter(number_locations == 7) %>% 
  knitr::kable()
```

### Part 1.2 

Spaghetti plot that shows the number of locations in each state from 2002 to 2010

```{r}
brfss %>% 
  group_by(state, year) %>% 
  distinct(state, location) %>% 
  summarise(number_locations = n()) %>% 
  ggplot(aes(x = year, y = number_locations, color = state)) +
    geom_line() +
    labs(
      title = "Number of Locations Represented in State",
      x = "Year",
      y = "Number of Locations",
      caption = "Data from the BRFSS dataset"
    ) + 
    viridis::scale_color_viridis(
      name = "State", 
      discrete = TRUE) +
    theme_bw() +  
    theme(legend.position = "bottom")
```

### Part 1.3 

In NY State, the average proportions of "Excellent" responses on the Overall Health questionnaire were 24.04%, 22.53%, and 22.70% in years 2002, 2006, and 2010, respectively. The standard deviation for proportion of "Excellent" responses were 4.49, 4.00, 3.57 in years 2002, 2006, and 2010, respectively.

```{r}
brfss %>% 
  filter(
    year %in% c(2002, 2006, 2010), 
    response == "Excellent",
    state == "NY") %>% 
  group_by(year) %>% 
  summarise(
    excellent_mean = mean(proportion),
    excellent_sd = sd(proportion)
  ) %>% 
  knitr::kable(digits = 2)

ungroup(brfss)
```

### Part 1.4 

```{r, collapse=TRUE}
mean_response = brfss %>% 
  group_by(year, state, response) %>% 
  summarise(avg_response = mean(proportion))

mean_response %>% 
  ggplot(aes(x = year, y = avg_response, color = state)) +
    geom_point(alpha = 0.5) +
    labs(
      title = "State-Level Response Proportions by Year",
      x = "Year",
      y = "Average Proportion (%)",
      caption = "Data from the BRFSS dataset"
    ) + 
    facet_grid(~response) +   
    viridis::scale_color_viridis(
      name = "State", 
      discrete = TRUE) +
    theme_bw() +  
    theme(legend.position = "bottom")
```

# Problem 2

### Part 2.1

* There are 134 distinct aisles
* The top ten aisles with the most number of items ordered are shown in table below. 

Load & clean data
```{r, collapse=TRUE}
instacart = p8105.datasets::instacart

#1. number of distinct aisles
nrow(distinct(instacart, aisle))

#2. Top ten aisles with the most number of items ordered
instacart %>% 
  group_by(aisle_id, aisle) %>% 
  summarise(items = n()) %>% 
  arrange(desc(items)) %>% 
  select(aisle, items) %>% 
  head(10) %>% 
  knitr::kable()
```

### Part 2.2

Plot of number of items ordered by aisle
```{r}
#Plot of number of items ordered by aisle
instacart %>% 
  group_by(aisle) %>% 
  summarise(items = n()) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, desc(items))) %>% 
  ggplot(aes(x = aisle, y = items)) +
    geom_col(aes(fill = aisle), alpha = 0.5) +
    theme_bw() + 
    theme(
      axis.text.x = element_text(angle = 90, size = rel(0.5)), 
      legend.position = "none") +
    labs(
      title = "Number of Items Ordered by Aisle",
      x = "Aisle",
      y = "Number of Items",
      caption = "*Data from Instacart")
```


### Part 2.3

* The most popular item in the baking ingredients aisle is light brown sugar, present in 499 orders.
* The most popular item in the dog food care aisle is Snack Sticks Chicken and Rice Recipe Dog Treats, present in 30 orders
* The most popular item in the packaged vegetables fruits aisle is Organic Baby Spinach, present in 9784 orders.

Table displays the most popular item in the “baking ingredients”, “dog food care”, and “packaged vegetables fruits” aisles.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(number_orders = n()) %>% 
  ungroup() %>%
  group_by(aisle) %>% 
  mutate(order_rank = min_rank(desc(number_orders))) %>% 
  filter(order_rank == 1) %>% 
  knitr::kable()
```

```{r, include=FALSE}
#ungroup instacart dataset
ungroup(instacart)
```

### Part 2.4

Table displays the mean hour in each day of the week that coffee ice cream and pink lady apples are ordered. For both items, mean order hour falls slightly before noon to early afternoon. This suggests that approximately half of orders are placed before noon and half of orders are placed after. For coffee ice cream, the mean order time is latest midweek from Tues-Thurs around 3pm. 

*In the instacart dataset, we are taking the assumption that values 0-6 in the order_dow variable correspond to days Sun-Sat. 

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_order_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  spread(key = order_dow, value = mean_order_hour) %>% 
  rename(Sun = "0", Mon = "1", Tues = "2", Wed = "3", Thur = "4", Fri = "5", Sat = "6") %>% 
  knitr::kable(digits = 2)
```

# Problem 3

* The `ny_noaa` dataset is extracted from the National Oceanic and Atmospheric Administration
* contains weather parameters from NY weather monitors
* Date ranges Jan 1st, 1981 to Dec 31st, 2010
* Data present 747 monitors.
* Variable names in dataset correspond to the following parameters: prcp = precipitation (tenths of mm), snow = snowfall (mm), snwd = snow depth (mm), tmax = maximum temperature (tenths of Cº), tmin = minimum temperature (tenths of Cº).
* Missing datapoints presents a significant issue for analysis. The following proportions of each variable is NA:
    * prcp: 145838 NAs (`r 145838/2595176`%)
    * snow: 381221 NAs (`r 381221/2595176`%)
    * snwd: 591786 NAs (`r 591786/2595176`%)
    * tmax: 1134358 NAs (`r 1134358/2595176`%)
    * tmin: 1134420 NAs (`r 1134420/2595176`%)

Data Cleaning:

* Convert prcp, snow, snwd, tmax, and tmin to integers
* Convert units for prcp to "mm", tmin to ºC, tmax to ºC.
* separate date into year, month, day

```{r, collapse=TRUE}
#load dataset
ny_noaa = p8105.datasets::ny_noaa

#dataset cleaning
noaa_tidy = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    prcp = as.integer(prcp)/10,
    snow = as.integer(snow),
    snwd = as.integer(snwd),
    tmax = as.integer(tmax)/10,
    tmin = as.integer(tmin)/10, 
    month = as.integer(month), 
    month = month.abb[month])
  
#summary of NAs in each variable
summary(noaa_tidy)

#number of monitor IDs
noaa_tidy %>% 
  distinct(id) %>% 
  nrow()
```

### Part 3.1

Table displays the top 6 most commonly observed values for snowfall. 0mm of snowfall is the most commonly observed value, which indicates that there was no snowfall on the majority of days observed by the weather monitor. The second most observed value is "NA", which is consistent with the fact that 14.6% of the observed days are missing data points for snowfall. 

```{r}
noaa_tidy %>%
  group_by(snow) %>% 
  summarise(frequency = n()) %>% 
  arrange(desc(frequency)) %>% 
  rename("snowfall(mm)" = "snow") %>% 
  head() %>% 
  knitr::kable()
```

```{r, include=FALSE}
ungroup(noaa_tidy)
```

### Part 3.2

```{r}
noaa_tidy %>% 
  filter(month %in% c("Jan", "Jul")) %>% 
  group_by(year, month, id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = avg_tmax, group = year)) +
    geom_boxplot() +
    facet_grid(~month) +
    scale_x_discrete(breaks = c(1970, 1980, 1990, 2000, 2010)) +
  labs(
    title = "Distribution of max temp from 1981 to 2010",
    x = "Year",
    y = "Average max temperature of monitor (ºC)",
    caption = "Data from noaa") +
  theme_bw()
```

### Part 3.3

Two panel-plot of tmax vs. tmin 

```{r}
tmax_tmin = noaa_tidy %>% 
  ggplot(aes(x = tmax, y = tmin)) +
    geom_hex() +
    labs(
      title = "Maximum versus Minimum Temperature, 1981-2010",
      x = "Maxiumum daily temperature (ºC)",
      y = "Minimum daily temperature (ºC)")

noaa_tidy %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = snow, y = year, fill = snow)) 
```

